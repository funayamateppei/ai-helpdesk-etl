import json
import numpy as np
import os
from typing import List
import google.generativeai as genai
from dotenv import load_dotenv
from text_normalizer import get_gemini_embedding
from sklearn.decomposition import PCA

# --- .envã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ ---
load_dotenv()

# --- è¨­å®š ---
VECTOR_FILE = "vectorized_chunks.json"
SIM_THRESHOLD = 0.7

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
def cosine_similarity(a: List[float], b: List[float]) -> float:
    a = np.array(a)
    b = np.array(b)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

def get_embedding(text: str) -> List[float]:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    return get_gemini_embedding(text, GEMINI_API_KEY, task_type="retrieval_document")

def find_most_similar(query_emb: List[float], chunks: List[dict]):
    sims = [cosine_similarity(query_emb, c["embedding"]) for c in chunks]
    print(f"é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {sims}")
    max_idx = int(np.argmax(sims))
    return chunks[max_idx], sims[max_idx]

def ask_llm(context: str, question: str) -> str:
    prompt = f"""
ã‚ãªãŸã¯ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸botã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã¯ãªã„ç¤¾å†…ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦ä¸å¯§ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
---
å‚è€ƒæƒ…å ±:
{context}
---
è³ªå•: {question}
å›ç­”:
"""
    model = genai.GenerativeModel("gemini-1.5-flash-latest")
    response = model.generate_content(prompt)
    return response.text.strip()

def save_question_vector(question: str, embedding: list, path: str = "question_vectors.json"):
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®embeddingã‚’å…¨éƒ¨èª­ã¿è¾¼ã‚€
    with open("vectorized_chunks.json", encoding="utf-8") as f:
        doc_data = json.load(f)
    all_embeddings = [item["embedding"] for item in doc_data] + [embedding]
    pca = PCA(n_components=3)
    reduced = pca.fit_transform(all_embeddings)
    # è³ªå•ã¯æœ€å¾Œ
    qx, qy, qz = reduced[-1][0], reduced[-1][1], reduced[-1][2]
    output = {
        'question': question,
        'x': float(qx),
        'y': float(qy),
        'z': float(qz)
    }
    with open('question_vectors_3d.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    data_q = {
        "question": question,
        "embedding": embedding
    }
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data_q, f, ensure_ascii=False, indent=2)

def main():
    print("è³ªå•ã‚’ã©ã†ã (Ctrl+Cã§çµ‚äº†)")
    with open(VECTOR_FILE, encoding="utf-8") as f:
        chunks = json.load(f)
    while True:
        try:
            question = input("\nQ: ")
            query_emb = get_embedding(question)
            save_question_vector(question, query_emb)  # è³ªå•å†…å®¹ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜
            best, sim = find_most_similar(query_emb, chunks)
            if sim < SIM_THRESHOLD:
                print("A: ãã®è³ªå•ã¯ã¾ã ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ã•ã‚Œã¦ã„ãªã„ã‚ˆã†ã§ã™ã€‚")
                continue
            answer = ask_llm(best["chunk"], question)
            print(f"A: {answer}\n(å‚è€ƒ: {best['url']})")
        except KeyboardInterrupt:
            print("\nã°ã„ã°ã„ğŸ‘‹")
            break
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
