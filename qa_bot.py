import json
import numpy as np
import os
from typing import List
import google.generativeai as genai
from dotenv import load_dotenv

# --- .envã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ ---
load_dotenv()

# --- è¨­å®š ---
VECTOR_FILE = "vectorized_chunks.json"
SIM_THRESHOLD = 0.5

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
def cosine_similarity(a: List[float], b: List[float]) -> float:
    a = np.array(a)
    b = np.array(b)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

def get_embedding(text: str) -> List[float]:
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY or GEMINI_API_KEY.strip() == "":
        print("[è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼] GEMINI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼.envã‚„ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ã­ï¼")
        exit(1)
    genai.configure(api_key=GEMINI_API_KEY)
    try:
        response = genai.embed_content(
            model="models/text-embedding-004",
            content=text,
            task_type="retrieval_query"
        )
        return response["embedding"]
    except Exception as e:
        print(f"Gemini embeddingå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def find_most_similar(query_emb: List[float], chunks: List[dict]):
    sims = [cosine_similarity(query_emb, c["embedding"]) for c in chunks]
    max_idx = int(np.argmax(sims))
    return chunks[max_idx], sims[max_idx]

def ask_llm(context: str, question: str) -> str:
    prompt = f"""
ã‚ãªãŸã¯ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸botã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã€ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã¯ãªã„ç¤¾å†…ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦ä¸å¯§ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
---
å‚è€ƒæƒ…å ±:
{context}
---
è³ªå•: {question}
å›ç­”:
"""
    model = genai.GenerativeModel("gemini-1.5-flash-latest")
    response = model.generate_content(prompt)
    return response.text.strip()

def save_question_vector(question: str, embedding: list, path: str = "question_vectors.json"):
    import os
    import json
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    else:
        data = []
    data.append({
        "question": question,
        "embedding": embedding
    })
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def main():
    print("è³ªå•ã‚’ã©ã†ã (Ctrl+Cã§çµ‚äº†)")
    with open(VECTOR_FILE, encoding="utf-8") as f:
        chunks = json.load(f)
    while True:
        try:
            question = input("\nQ: ")
            query_emb = get_embedding(question)
            save_question_vector(question, query_emb)  # è³ªå•å†…å®¹ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜
            best, sim = find_most_similar(query_emb, chunks)
            if sim < SIM_THRESHOLD:
                print("A: ãã®è³ªå•ã¯ã¾ã ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ã•ã‚Œã¦ã„ãªã„ã‚ˆã†ã§ã™ã€‚")
                continue
            answer = ask_llm(best["chunk"], question)
            print(f"A: {answer}\n(å‚è€ƒ: {best['url']})")
        except KeyboardInterrupt:
            print("\nã°ã„ã°ã„ğŸ‘‹")
            break
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
